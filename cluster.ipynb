{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/mario/spark-3.3.1-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, year, month, concat_ws\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType, DateType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 09:07:10 WARN Utils: Your hostname, mario-hpprobook450g5 resolves to a loopback address: 127.0.1.1; using 192.168.144.80 instead (on interface wlp3s0)\n",
      "23/02/03 09:07:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 09:07:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"myApp\").setMaster(\"spark://192.168.144.80:7077\").set('spark.executor.memory', '8g').set('spark.driver.memory', '8g')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup(\"job group id\", \"read all files from HDFS and print the schema\")\n",
    "folder_Temperature = 'hdfs://192.168.144.80:9000/user/mario/input_files/temperatur_only_10_stations/'\n",
    "folder_Solar = 'hdfs://192.168.144.80:9000/user/mario/input_files/solar_only_10_stations/'\n",
    "\n",
    "#Einlesen der Daten in einen DataFrame\n",
    "\n",
    "#Lufttemperatur\n",
    "#Daniel\n",
    "df_Temperatur = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"false\").csv(folder_Temperature+'*.txt')\n",
    "#Sonnenscheindauer\n",
    "#Daniel\n",
    "df_Solar = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"false\").csv(folder_Solar+'*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Temperatur.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Solar.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nächste Schritte: \n",
    "1. Datenstruktur verstehen \n",
    "2. Daten bearbeiten und in einen finalen DF schreiben \n",
    "-> Zusammenfassen der Daten auf eine Station und einen TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- PP_10: string (nullable = true)\n",
      " |-- TT_10: string (nullable = true)\n",
      " |-- TM5_10: string (nullable = true)\n",
      " |-- RF_10: string (nullable = true)\n",
      " |-- TD_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Temperatur.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus diesem Datensatz benötigen wir die Spalten StationsID und MESS_Datum (Zeitstempel) TT_10 (Luftemperatur in 2m Höhe) und TM5_10 (Luftemperatur in 2cm Höhe) und PP_10(Luftdruck)\n",
    "Ändern der Datentypen von TT_10, TM5_10 und PP_10 in float, MESS_Datum in Timestamp ändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+---------+-------------+--------------+\n",
      "|STATIONS_ID|MESS_DATUM         |Luftdruck|Temperatur_2m|Temperatur_5cm|\n",
      "+-----------+-------------------+---------+-------------+--------------+\n",
      "|1766       |2010-01-01 00:00:00|989.7    |-1.3         |-1.8          |\n",
      "|1766       |2010-01-01 00:10:00|989.7    |-1.3         |-1.8          |\n",
      "|1766       |2010-01-01 00:20:00|989.8    |-1.3         |-1.8          |\n",
      "+-----------+-------------------+---------+-------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bearbeiten der Struktur in Gewünschtes Format \n",
    "#Datentypen bearbeiten \n",
    "sc.setJobGroup(\"job group id\", \"select only columns that we need, change column type, rename columns\")\n",
    "df_Temperatur = df_Temperatur\\\n",
    "    .withColumn('STATIONS_ID',df_Temperatur.STATIONS_ID.cast(IntegerType()))\\\n",
    "    .withColumn('TM5_10',df_Temperatur.TM5_10.cast(FloatType()))\\\n",
    "    .withColumn('PP_10',df_Temperatur.PP_10.cast(FloatType()))\\\n",
    "    .withColumn('TT_10',df_Temperatur.TT_10.cast(FloatType()))\\\n",
    "    .withColumn(\"MESS_DATUM\",to_timestamp(\"MESS_DATUM\", \"yyyyMMddHHmm\"))\n",
    "\n",
    "#Spalten umbennen\n",
    "df_Temperatur = df_Temperatur\\\n",
    "    .withColumnRenamed('TT_10','Temperatur_2m')\\\n",
    "    .withColumnRenamed('TM5_10','Temperatur_5cm')\\\n",
    "    .withColumnRenamed('PP_10','Luftdruck')\n",
    "\n",
    "#Daten droppen\n",
    "df_Temperatur = df_Temperatur.drop('  QN','RF_10','TD_10','eor')\n",
    "\n",
    "sc.setJobGroup(\"job group id\", \"remove -999 values\")\n",
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Luftdruck != -999.0))\n",
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Temperatur_2m != -999.0))\n",
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Temperatur_5cm != -999.0))\n",
    "df_Temperatur.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Temperatur.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- DS_10: string (nullable = true)\n",
      " |-- GS_10: string (nullable = true)\n",
      " |-- SD_10: string (nullable = true)\n",
      " |-- LS_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Solar.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus diesem Datensatz benötigen wir die Spalten StationsID und MESS_Datum (Zeitstempel), SD_10 (10 min-Summe der Sonnenscheindauer)\n",
    "Ändern der Datentypen von SD_10 in float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datentypen bearbeiten\n",
    "sc.setJobGroup(\"job group id\", \"select only columns that we need, change column type, rename columns\")\n",
    "df_Solar = df_Solar\\\n",
    "    .withColumn('SD_10',df_Solar.SD_10.cast(FloatType()))\\\n",
    "    .withColumn(\"MESS_DATUM\",to_timestamp(\"MESS_DATUM\", \"yyyyMMddHHmm\"))\n",
    "#Daten droppen \n",
    "df_Solar = df_Solar.drop('  QN','DS_10','GS_10','LS_10','eor')\n",
    "#Spalte umbennen\n",
    "df_Solar = df_Solar.withColumnRenamed('SD_10','Sonnenscheindauer')\n",
    "df_Solar = df_Solar.filter((df_Solar.Sonnenscheindauer != -999.0))\n",
    "#df_Solar.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Solar.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die beiden Datensätze in die passende Struktur gebracht wurden, können Sie mit einem Join verbunden werden. \n",
    "Welchen Join? \n",
    "Da für die Auswertung des Spruches, für jede Station und jeden TimeStamp Daten von Solar und Temperature benötigt werden, müssen diese auch von beiden vorhanden sein. \n",
    "Daher wird der Inner Join gewählt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join der DF\n",
    "sc.setJobGroup(\"job group id\", \"join solar and temperature\")\n",
    "df_Final = df_Temperatur.join(df_Solar, (df_Temperatur.STATIONS_ID == df_Solar.STATIONS_ID) & (df_Temperatur.MESS_DATUM == df_Solar.MESS_DATUM),\"inner\").select(df_Temperatur['*'],df_Solar.Sonnenscheindauer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: timestamp (nullable = true)\n",
      " |-- Luftdruck: float (nullable = true)\n",
      " |-- Temperatur_2m: float (nullable = true)\n",
      " |-- Temperatur_5cm: float (nullable = true)\n",
      " |-- Sonnenscheindauer: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Final.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Final.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf der Basis dieser Datentabelle kann nun die Auswertung zur Bauernregel erfolgen. \n",
    "Diese Tabelle kann natürlich noch um weitere Wetterdaten erweitert werden, damit andere Bauernregeln oder andere Wettersimulationen durchgeführt werden können."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bauernregel 1:\n",
    "\n",
    "\"Ist der Januar hell und weiß, wird der Sommer sicher heiß\"\n",
    "1. DF anlegen, welcher alle Kombinationen von Station ID und Jahre besitzt\n",
    "2. Daten für Januar und Sommer erstellen\n",
    "3. Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: timestamp (nullable = true)\n",
      " |-- Luftdruck: float (nullable = true)\n",
      " |-- Temperatur_2m: float (nullable = true)\n",
      " |-- Temperatur_5cm: float (nullable = true)\n",
      " |-- Sonnenscheindauer: float (nullable = true)\n",
      " |-- Jahr: integer (nullable = true)\n",
      " |-- Monat: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DF aus dem df_Final erzeugen welcher alle Kombinationen der StationsIDs, Monate und Jahre enthält\n",
    "#Neue Spalte Jahr hinzufügen\n",
    "df = df_Final\n",
    "df  = df.withColumn(\"Jahr\", year(\"MESS_DATUM\"))\n",
    "df  = df.withColumn(\"Monat\", month(\"MESS_DATUM\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = false)\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- Jahr: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setJobGroup(\"job group id\", \"merge 2 columns into 1\")\n",
    "df_Kombinationen =  df.withColumn(\"ID\",concat_ws('','Jahr','STATIONS_ID'))\n",
    "df_Kombinationen = df_Kombinationen.select(\"ID\",'STATIONS_ID','Jahr')\n",
    "df_Kombinationen = df_Kombinationen.dropDuplicates([\"ID\"])\n",
    "df_Kombinationen.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Kombinationen.show(3,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro Jahr sind nur die Temperaturen von 21. Juni bis 23. September und die Sonnenscheindauer im Januar wichtig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zu DF df_Kombinationen eine neue Spalte hinzufügen, welche die Summe an Sonnenstunden pro StationsID und Jahr enthält\n",
    "sc.setJobGroup(\"job group id\", \"filter,groupBy,mean\")\n",
    "df_Final_Solar_Grouped = df.filter((df.Monat == 12)).groupBy('STATIONS_ID','Jahr','Monat').mean('Sonnenscheindauer')\n",
    "df_Final_Solar_Grouped = df_Final_Solar_Grouped.withColumnRenamed('avg(Sonnenscheindauer)','Sonnenscheindauer')\n",
    "sc.setJobGroup(\"job group id\", \"join 2 dfs\")\n",
    "df_Kombinationen = df_Kombinationen.join(df_Final_Solar_Grouped,(df_Kombinationen.STATIONS_ID == df_Final_Solar_Grouped.STATIONS_ID)&(df_Kombinationen.Jahr == df_Final_Solar_Grouped.Jahr )).select(df_Kombinationen['*'],df_Final_Solar_Grouped.Sonnenscheindauer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"job group id\", \"filter, groupBy, mean\")\n",
    "df_Final_Temperatur_Grouped = df.filter((df.Monat == 7)|(df.Monat == 8)).groupBy('STATIONS_ID','Jahr').mean('Temperatur_2m')\n",
    "df_Final_Temperatur_Grouped = df_Final_Temperatur_Grouped.withColumnRenamed('avg(Temperatur_2m)','Temperatur_2m')\n",
    "sc.setJobGroup(\"job group id\", \"join 2 dfs\")\n",
    "df_Kombinationen = df_Kombinationen.join(df_Final_Temperatur_Grouped,(df_Kombinationen.STATIONS_ID == df_Final_Temperatur_Grouped.STATIONS_ID)&(df_Kombinationen.Jahr == df_Final_Temperatur_Grouped.Jahr )).select(df_Kombinationen['*'],df_Final_Temperatur_Grouped.Temperatur_2m)\n",
    "df_Kombinationen = df_Kombinationen.sort('Jahr')\n",
    "df_Kombinationen = df_Kombinationen.sort('STATIONS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = false)\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- Jahr: integer (nullable = true)\n",
      " |-- Sonnenscheindauer: double (nullable = true)\n",
      " |-- Temperatur_2m: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Kombinationen.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Kombinationen.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==>(17 + 8) / 25][Stage 8:==>(21 + 3) / 24][Stage 9:>   (0 + 9) / 25]4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 09:07:50 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.144.80: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 11.0 in stage 7.0 (TID 158) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 13.0 in stage 7.0 (TID 160) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 7.0 in stage 7.0 (TID 154) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 148) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 9.0 in stage 7.0 (TID 156) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 3.0 in stage 7.0 (TID 150) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 15.0 in stage 7.0 (TID 162) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:07:50 WARN TaskSetManager: Lost task 5.0 in stage 7.0 (TID 152) (192.168.144.80 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==>(17 + 0) / 25][Stage 10:=>(20 + 4) / 24][Stage 11:>  (0 + 8) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 09:08:03 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.144.80: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 22.0 in stage 9.0 (TID 226) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 16.0 in stage 9.0 (TID 220) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 19.0 in stage 9.0 (TID 223) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 21.0 in stage 9.0 (TID 225) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 15.0 in stage 9.0 (TID 219) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 18.0 in stage 9.0 (TID 222) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 17.0 in stage 9.0 (TID 221) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/02/03 09:08:03 WARN TaskSetManager: Lost task 20.0 in stage 9.0 (TID 224) (192.168.144.80 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Korrelation zwischen Sonnenschein und Durschsnittstemperatur berechnen\n",
    "sc.setJobGroup(\"job group id\", \"calculate correlation\")\n",
    "corr_Spruch = df_Kombinationen.stat.corr('Sonnenscheindauer','Temperatur_2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07188381758199237\n"
     ]
    }
   ],
   "source": [
    "print(corr_Spruch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bauernregel 2:\n",
    "\n",
    "„Wenn's im Dezember nicht wintert, sommert's im Juni auch nicht.“\n",
    "\n",
    "Hierbei handelt es sich im Prinzip um die entgegengesetzte Regel zu der vorherigen:\n",
    "Im Juni wird es nicht warm, wenn im Dezember kein richtiger Winter war.\n",
    "Es stimmt, dass die These zu 65 Prozent eintrifft. Andersrum, wenn es also im Juni nicht „sommert“, wird es im Dezember nicht „wintern“,\n",
    "stimmt die Bauernregel aber nicht. Warum das so ist, weiß keiner so genau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need months June and Decemeber\n",
    "sc.setJobGroup(\"job group id\", \"filter only june and december\")\n",
    "df = df_Final\n",
    "df = df.filter((month(\"MESS_DATUM\") == 6) | (month(\"MESS_DATUM\") == 12))\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"job group id\", \"convert timestamp into month and year\")\n",
    "df = df.select(\"STATIONS_ID\", year(\"MESS_DATUM\").alias(\"year\"),month(\"MESS_DATUM\").alias(\"month\"), \"Temperatur_2m\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average temperature for each year+month+stations_id\n",
    "sc.setJobGroup(\"job group id\", \"calculate average temperature for each year+month+stations_id\")\n",
    "df = df.groupBy(\"year\", \"month\", \"STATIONS_ID\").avg(\"Temperatur_2m\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"job group id\", \"new dataframe with only december\")\n",
    "df2 = df.filter(df.month == 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "sc.setJobGroup(\"job group id\", \"adding additional month and year columns\")\n",
    "df2 = df2.withColumnRenamed('avg(Temperatur_2m)', 'temperature_december')\n",
    "df2 = df2.withColumnRenamed('year', 'year_december')\n",
    "df2 = df2.withColumnRenamed('month', 'month_december')\n",
    "df2 = df2.withColumn('year_june', df2.year_december + 1)\n",
    "df2 = df2.withColumn('month_june', lit(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"job group id\", \"join two dataframes\")\n",
    "df2 = df2.join(df, (df2.year_june == df.year) & (df2.month_june == df.month) & (df2.STATIONS_ID == df.STATIONS_ID)).select(df2[\"*\"], df[\"avg(Temperatur_2m)\"])\n",
    "df2 = df2.withColumnRenamed('avg(Temperatur_2m)', 'temperature_june')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=> (5 + 8) / 13][Stage 53:>  (0 + 4) / 13][Stage 56:>  (0 + 0) / 12]3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 09:08:54 WARN StandaloneAppClient$ClientEndpoint: Connection to 192.168.144.80:7077 failed; waiting for master to reconnect...\n",
      "23/02/03 09:08:54 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "23/02/03 09:08:54 WARN StandaloneAppClient$ClientEndpoint: Connection to 192.168.144.80:7077 failed; waiting for master to reconnect...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2159190300045076"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.setJobGroup(\"job group id\", \"calculate correlation\")\n",
    "df2.corr(\"temperature_december\", \"temperature_june\", method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/hdfs://192.168.199.80:9000/user/mario/output_files/10minutenwerte_only_100_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
