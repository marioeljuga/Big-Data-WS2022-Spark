{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook sind die Ergebnisse des BigData Projektes der Gruppe 5 zusammengefasst. \n",
    "Als Analyseobjekt dient die Wettersimulation. Hierfür wird auf der Basis von Wetterdaten des Deutschen Wetterdienstes (DWD) als Beispiel die Bauernregel \"Ist der Januar hell und weiß, wird der Sommer sicher heiß\" auf seine Korrektheit überprüft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/31 17:27:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://minivondaniel2.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigData</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10bb05c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up von Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName(\"BigData\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speicherorte der Daten festlegen \n",
    "#Daniel\n",
    "folder_Temperature = '/Users/danielwentsch/Desktop/BigData/Data/02_airtemperature/'\n",
    "folder_Solar = '/Users/danielwentsch/Desktop/BigData/Data/04_solar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Einlesen der Daten in einen DataFrame\n",
    "\n",
    "#Lufttemperatur\n",
    "#Daniel\n",
    "df_Temperatur = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"false\").csv(folder_Temperature+'*.txt')\n",
    "#Sonnenscheindauer\n",
    "#Daniel\n",
    "df_Solar = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"false\").csv(folder_Solar+'*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134183347"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temperatur.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89999273"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Solar.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nächste Schritte: \n",
    "1. Datenstruktur verstehen \n",
    "2. Daten bearbeiten und in einen finalen DF schreiben \n",
    "-> Zusammenfassen der Daten auf eine Station und einen TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- PP_10: string (nullable = true)\n",
      " |-- TT_10: string (nullable = true)\n",
      " |-- TM5_10: string (nullable = true)\n",
      " |-- RF_10: string (nullable = true)\n",
      " |-- TD_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Temperatur.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus diesem Datensatz benötigen wir die Spalten StationsID und MESS_Datum (Zeitstempel) TT_10 (Luftemperatur in 2m Höhe) und TM5_10 (Luftemperatur in 2cm Höhe) und PP_10(Luftdruck)\n",
    "Ändern der Datentypen von TT_10, TM5_10 und PP_10 in float, MESS_Datum in Time ändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+---------+-------------+--------------+\n",
      "|STATIONS_ID|MESS_DATUM         |Luftdruck|Temperatur_2m|Temperatur_5cm|\n",
      "+-----------+-------------------+---------+-------------+--------------+\n",
      "|1468       |2010-01-01 00:00:00|899.8    |3.2          |2.8           |\n",
      "|1468       |2010-01-01 00:10:00|899.7    |3.2          |2.7           |\n",
      "|1468       |2010-01-01 00:20:00|899.6    |2.9          |2.2           |\n",
      "+-----------+-------------------+---------+-------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bearbeiten der Struktur in Geünschtes Format \n",
    "#Datentypen bearbeiten \n",
    "df_Temperatur = df_Temperatur.withColumn('STATIONS_ID',df_Temperatur.STATIONS_ID.cast(IntegerType())).withColumn('TM5_10',df_Temperatur.TM5_10.cast(FloatType())).withColumn('PP_10',df_Temperatur.PP_10.cast(FloatType())).withColumn('TT_10',df_Temperatur.TT_10.cast(FloatType())).withColumn(\"MESS_DATUM\",to_timestamp(\"MESS_DATUM\", \"yyyyMMddHHmm\"))\n",
    "#Daten droppen\n",
    "df_Temperatur = df_Temperatur.drop('  QN','RF_10','TD_10','eor')\n",
    "#Spalten umbennen\n",
    "df_Temperatur = df_Temperatur.withColumnRenamed('TT_10','Temperatur_2m').withColumnRenamed('TM5_10','Temperatur_5cm').withColumnRenamed('PP_10','Luftdruck')\n",
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Luftdruck != -999.0))\n",
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Temperatur_2m != -999.0))\n",
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Temperatur_5cm != -999.0))\n",
    "df_Temperatur.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67198992"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temperatur.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- DS_10: string (nullable = true)\n",
      " |-- GS_10: string (nullable = true)\n",
      " |-- SD_10: string (nullable = true)\n",
      " |-- LS_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Solar.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus diesem Datensatz benötigen wir die Spalten StationsID und MESS_Datum (Zeitstempel), SD_10 (10 min-Summe der Sonnenscheindauer)\n",
    "Ändern der Datentypen von SD_10 in float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+\n",
      "|STATIONS_ID|MESS_DATUM         |Sonnenscheindauer|\n",
      "+-----------+-------------------+-----------------+\n",
      "|       1048|2010-01-01 00:00:00|0.0              |\n",
      "|       1048|2010-01-01 00:10:00|0.0              |\n",
      "|       1048|2010-01-01 00:20:00|0.0              |\n",
      "+-----------+-------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Datentypen bearbeiten\n",
    "df_Solar = df_Solar.withColumn('SD_10',df_Solar.SD_10.cast(FloatType())).withColumn(\"MESS_DATUM\",to_timestamp(\"MESS_DATUM\", \"yyyyMMddHHmm\"))\n",
    "#Daten droppen \n",
    "df_Solar = df_Solar.drop('  QN','DS_10','GS_10','LS_10','eor')\n",
    "#Spalte numbennen\n",
    "df_Solar = df_Solar.withColumnRenamed('SD_10','Sonnenscheindauer')\n",
    "df_Solar = df_Solar.filter((df_Solar.Sonnenscheindauer != -999.0))\n",
    "df_Solar.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86961897"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Solar.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die beiden Datensätze in die passende Struktur gebracht wurden, können Sie mit einem Join verbunden werden. \n",
    "Welchen Join? \n",
    "Da für die Auswertung des Spruches, für jede Station und jeden TimeStamp Daten von Solar und Temperature benötigt werden, müssen diese auch von beiden vorhanden sein. \n",
    "Daher wird der Inner Join gewählt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join der DF\n",
    "df_Final = df_Temperatur.join(df_Solar, (df_Temperatur.STATIONS_ID == df_Solar.STATIONS_ID) & (df_Temperatur.MESS_DATUM == df_Solar.MESS_DATUM),\"inner\").select(df_Temperatur['*'],df_Solar.Sonnenscheindauer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: timestamp (nullable = true)\n",
      " |-- Luftdruck: float (nullable = true)\n",
      " |-- Temperatur_2m: float (nullable = true)\n",
      " |-- Temperatur_5cm: float (nullable = true)\n",
      " |-- Sonnenscheindauer: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+---------+-------------+--------------+-----------------+\n",
      "|STATIONS_ID|MESS_DATUM         |Luftdruck|Temperatur_2m|Temperatur_5cm|Sonnenscheindauer|\n",
      "+-----------+-------------------+---------+-------------+--------------+-----------------+\n",
      "|3          |1993-04-28 16:40:00|985.6    |26.1         |27.2          |0.0              |\n",
      "|3          |1993-04-28 16:40:00|985.6    |26.1         |27.2          |0.0              |\n",
      "|3          |1993-04-29 10:20:00|987.7    |22.0         |25.5          |0.0              |\n",
      "+-----------+-------------------+---------+-------------+--------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_Final.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63917012"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Final.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf der Basis dieser Datentabelle kann nun die Auswertung zur Bauernregel erfolgen. \n",
    "Diese Tabelle kann natürlich noch um weitere Wetterdaten erweitert werden, damit andere Bauernregeln oder andere Wettersimulationen durchgeführt werden können."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erfolgt die Auswertung des Spruches: \"Ist der Januar hell und weiß, wird der Sommer sicher heiß\"\n",
    "1. DF anlegen, welcher alle Kombinationen von Station ID und Jahre besitzt\n",
    "2. Daten für Januar und Sommer erstellen\n",
    "3. Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: timestamp (nullable = true)\n",
      " |-- Luftdruck: float (nullable = true)\n",
      " |-- Temperatur_2m: float (nullable = true)\n",
      " |-- Temperatur_5cm: float (nullable = true)\n",
      " |-- Sonnenscheindauer: float (nullable = true)\n",
      " |-- Jahr: string (nullable = true)\n",
      " |-- Monat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DF aus dem df_Final erzeugen welcher alle Kmbinationen der StationsIDs, Monate und Jahre enthält\n",
    "#Neue Spalte Jahr hinzufügen\n",
    "df_Final  = df_Final.withColumn(\"Jahr\", df_Temperatur.MESS_DATUM.substr(1,4))\n",
    "df_Final  = df_Final.withColumn(\"Monat\", df_Temperatur.MESS_DATUM.substr(5,2))\n",
    "df_Final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = false)\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- Jahr: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Kombinationen =  df_Final.withColumn(\"ID\",concat_ws('','Jahr','STATIONS_ID'))\n",
    "df_Kombinationen = df_Kombinationen.select(\"ID\",'STATIONS_ID','Jahr')\n",
    "df_Kombinationen = df_Kombinationen.dropDuplicates([\"ID\"])\n",
    "df_Kombinationen.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro Jahr sind nur die Temperaturen von 21. Juni bis 23. September und die Sonnenscheindauer im Januar wichtig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zu DF df_Kombinationen eine neue Spalte hinzufügen, welche die Summe an Sonnenstunden pro StationsID und Jahr enthält \n",
    "df_Final_Solar_Grouped = df_Final.filter((df_Final.Monat == 12)).groupBy('STATIONS_ID','Jahr','Monat').mean('Sonnenscheindauer')\n",
    "df_Final_Solar_Grouped = df_Final_Solar_Grouped.withColumnRenamed('avg(Sonnenscheindauer)','Sonnenscheindauer')\n",
    "df_Kombinationen = df_Kombinationen.join(df_Final_Solar_Grouped,(df_Kombinationen.STATIONS_ID == df_Final_Solar_Grouped.STATIONS_ID)&(df_Kombinationen.Jahr == df_Final_Solar_Grouped.Jahr )).select(df_Kombinationen['*'],df_Final_Solar_Grouped.Sonnenscheindauer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Final_Temperatur_Grouped = df_Final.filter((df_Final.Monat == 7)|(df_Final.Monat == 8)).groupBy('STATIONS_ID','Jahr').mean('Temperatur_2m')\n",
    "df_Final_Temperatur_Grouped = df_Final_Temperatur_Grouped.withColumnRenamed('avg(Temperatur_2m)','Temperatur_2m')\n",
    "df_Kombinationen = df_Kombinationen.join(df_Final_Temperatur_Grouped,(df_Kombinationen.STATIONS_ID == df_Final_Temperatur_Grouped.STATIONS_ID)&(df_Kombinationen.Jahr == df_Final_Temperatur_Grouped.Jahr )).select(df_Kombinationen['*'],df_Final_Temperatur_Grouped.Temperatur_2m)\n",
    "df_Kombinationen = df_Kombinationen.sort('Jahr')\n",
    "df_Kombinationen = df_Kombinationen.sort('STATIONS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = false)\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- Jahr: string (nullable = true)\n",
      " |-- Sonnenscheindauer: double (nullable = true)\n",
      " |-- Temperatur_2m: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Kombinationen.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>  (0 + 8) / 64][Stage 38:> (0 + 0) / 101][Stage 39:>  (0 + 0) / 64]4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/31 17:42:14 WARN TaskMemoryManager: Failed to allocate a page (2097152 bytes), try again.\n",
      "23/01/31 17:42:14 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:14 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:14 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (2097152 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (2097152 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:15 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:16 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/01/31 17:42:17 ERROR Executor: Exception in task 2.0 in stage 42.0 (TID 3629)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/01/31 17:42:17 ERROR Executor: Exception in task 3.0 in stage 42.0 (TID 3630)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/01/31 17:42:18 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n",
      "23/01/31 17:42:18 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n",
      "23/01/31 17:42:18 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n",
      "23/01/31 17:42:18 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n",
      "23/01/31 17:42:19 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 42.0 (TID 3630),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/01/31 17:42:19 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 42.0 (TID 3629),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/01/31 17:42:21 WARN TaskSetManager: Lost task 2.0 in stage 42.0 (TID 3629) (minivondaniel2.fritz.box executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "23/01/31 17:42:21 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6771bc31 rejected from java.util.concurrent.ThreadPoolExecutor@39e43479[Shutting down, pool size = 8, active threads = 8, queued tasks = 0, completed tasks = 3627]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2057)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:827)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1357)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "23/01/31 17:42:22 ERROR TaskSetManager: Task 2 in stage 42.0 failed 1 times; aborting job\n",
      "23/01/31 17:42:24 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7f039fb6\n",
      "java.io.FileNotFoundException: /private/var/folders/jz/6yt3pgtj6c3cgct_p9pcvzhw0000gn/T/blockmgr-f35dd3c6-2bd7-4484-9c8b-e8a9de37dfaf/27/temp_local_ca5ad4e2-2166-4608-914f-ecb4b24a47bd (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:291)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:234)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:291)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.<init>(UnsafeSorterSpillWriter.java:81)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:213)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:297)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:95)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:391)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:447)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:57)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "23/01/31 17:42:24 ERROR UnsafeExternalSorter: Unable to grow the pointer array\n",
      "23/01/31 17:42:24 ERROR Executor: Exception in task 1.0 in stage 42.0 (TID 3628): error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7f039fb6 : /private/var/folders/jz/6yt3pgtj6c3cgct_p9pcvzhw0000gn/T/blockmgr-f35dd3c6-2bd7-4484-9c8b-e8a9de37dfaf/27/temp_local_ca5ad4e2-2166-4608-914f-ecb4b24a47bd (No such file or directory)\n",
      "23/01/31 17:42:24 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n",
      "23/01/31 17:42:24 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@405663c\n",
      "java.io.FileNotFoundException: /private/var/folders/jz/6yt3pgtj6c3cgct_p9pcvzhw0000gn/T/blockmgr-f35dd3c6-2bd7-4484-9c8b-e8a9de37dfaf/16/temp_local_d7d20b78-572f-451d-bb91-2daf85fdf2f9 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:291)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:234)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:291)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.<init>(UnsafeSorterSpillWriter.java:81)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:213)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:297)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:95)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:391)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:447)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:57)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "23/01/31 17:42:24 ERROR UnsafeExternalSorter: Unable to grow the pointer array\n",
      "23/01/31 17:42:24 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3b0e5418\n",
      "java.io.FileNotFoundException: /private/var/folders/jz/6yt3pgtj6c3cgct_p9pcvzhw0000gn/T/blockmgr-f35dd3c6-2bd7-4484-9c8b-e8a9de37dfaf/26/temp_local_e61d1df2-abff-45eb-b506-8f663ff5bf33 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:291)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:234)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:291)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.<init>(UnsafeSorterSpillWriter.java:81)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:213)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:297)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:324)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:95)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:391)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:447)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:57)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "23/01/31 17:42:24 ERROR UnsafeExternalSorter: Unable to grow the pointer array\n",
      "23/01/31 17:42:24 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.lang.IllegalStateException: Block broadcast_49 not found\n",
      "\tat org.apache.spark.storage.BlockInfoManager.$anonfun$unlock$3(BlockInfoManager.scala:293)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:293)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1264)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1$adapted(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:130)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:142)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:142)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:197)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:142)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "23/01/31 17:42:25 ERROR Executor: Exception in task 5.0 in stage 42.0 (TID 3632): error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3b0e5418 : /private/var/folders/jz/6yt3pgtj6c3cgct_p9pcvzhw0000gn/T/blockmgr-f35dd3c6-2bd7-4484-9c8b-e8a9de37dfaf/26/temp_local_e61d1df2-abff-45eb-b506-8f663ff5bf33 (No such file or directory)\n",
      "23/01/31 17:42:25 ERROR Executor: Exception in task 0.0 in stage 42.0 (TID 3627): Block broadcast_49 not found\n",
      "\n",
      "Previous exception in task: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@405663c : /private/var/folders/jz/6yt3pgtj6c3cgct_p9pcvzhw0000gn/T/blockmgr-f35dd3c6-2bd7-4484-9c8b-e8a9de37dfaf/16/temp_local_d7d20b78-572f-451d-bb91-2daf85fdf2f9 (No such file or directory)\n",
      "\torg.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:230)\n",
      "\torg.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:297)\n",
      "\torg.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:95)\n",
      "\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:391)\n",
      "\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:447)\n",
      "\torg.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n",
      "\torg.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.sort_addToSorter_0$(Unknown Source)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n",
      "\torg.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:60)\n",
      "\torg.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:57)\n",
      "\torg.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\torg.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\torg.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\torg.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\torg.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:832)\n"
     ]
    }
   ],
   "source": [
    "#Korrelation zwischen Sonnenschein und Durschsnittstemperatur berechnen\n",
    "corr_Spruch = df_Kombinationen.stat.corr('Sonnenscheindauer','Temperatur_2m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
