{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#Local standalone mode; exekutoren;wie paralleliziert spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/13 19:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://minivondaniel2.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigData</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a2f2190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"BigData\").getOrCreate()\n",
    "#Quelle: https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speicherort der Stationsinformationen festlegen\n",
    "folder_Stations = '/Users/danielwentsch/Desktop/BigData/Data/01_Stations/'\n",
    "#Speicherort der Temperaturdaten\n",
    "folder_Temperature = '/Users/danielwentsch/Desktop/BigData/Data/02_air_temperature/'\n",
    "#Datenbeschreibung: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/air_temperature/historical/BESCHREIBUNG_obsgermany_climate_10min_tu_historical_de.pdf \n",
    "#Speicherort der Winddaten\n",
    "folder_Wind = '/Users/danielwentsch/Desktop/BigData/Data/03_wind/'\n",
    "#Speicherort der Sonnendaten\n",
    "folder_Solar = '/Users/danielwentsch/Desktop/BigData/Data/04_solar/'\n",
    "#Datenbeschreibung: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/solar/historical/BESCHREIBUNG_obsgermany_climate_10min_solar_historical_de.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Loading Temperature Data in df\n",
    "#Quelle = https://stackoverflow.com/questions/49471192/spark-2-3-0-read-text-file-with-header-option-not-working\n",
    "df_Temperatur = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"false\").csv(folder_Temperature+'*.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10977397"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Anzahl der Datens√§tze im DataFrame Temperature\n",
    "df_Temperatur.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+-------+------+------+------+------+---+\n",
      "|STATIONS_ID|MESS_DATUM  |  QN |PP_10  |TT_10 |TM5_10|RF_10 |TD_10 |eor|\n",
      "+-----------+------------+-----+-------+------+------+------+------+---+\n",
      "|       2171|201001010000|    3|  960.8|  -1.3|  -1.3|  94.5|  -2.1|eor|\n",
      "|       2171|201001010010|    3|  960.8|  -1.4|  -1.3|  94.6|  -2.2|eor|\n",
      "|       2171|201001010020|    3|  960.7|  -1.4|  -1.3|  94.9|  -2.1|eor|\n",
      "+-----------+------------+-----+-------+------+------+------+------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Struktur der DF Temperature\n",
    "df_Temperatur.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- PP_10: string (nullable = true)\n",
      " |-- TT_10: string (nullable = true)\n",
      " |-- TM5_10: string (nullable = true)\n",
      " |-- RF_10: string (nullable = true)\n",
      " |-- TD_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema des DF anzeigen\n",
    "df_Temperatur.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- PP_10: float (nullable = true)\n",
      " |-- TT_10: string (nullable = true)\n",
      " |-- TM5_10: string (nullable = true)\n",
      " |-- RF_10: string (nullable = true)\n",
      " |-- TD_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "df_Temperatur = df_Temperatur.withColumn('PP_10',df_Temperatur.PP_10.cast(FloatType()))\n",
    "df_Temperatur = df_Temperatur.withColumn('STATIONS_ID',df_Temperatur.STATIONS_ID.cast(IntegerType()))\n",
    "df_Temperatur.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+-----+------+------+------+------+---+----+-----+---+------+------+\n",
      "|STATIONS_ID|  MESS_DATUM|   QN|PP_10| TT_10|TM5_10| RF_10| TD_10|eor|Jahr|Monat|Tag|Stunde|Minute|\n",
      "+-----------+------------+-----+-----+------+------+------+------+---+----+-----+---+------+------+\n",
      "|       2171|201001010000|    3|960.8|  -1.3|  -1.3|  94.5|  -2.1|eor|2010|   01| 01|    00|    00|\n",
      "|       2171|201001010010|    3|960.8|  -1.4|  -1.3|  94.6|  -2.2|eor|2010|   01| 01|    00|    10|\n",
      "|       2171|201001010020|    3|960.7|  -1.4|  -1.3|  94.9|  -2.1|eor|2010|   01| 01|    00|    20|\n",
      "|       2171|201001010030|    3|960.8|  -1.4|  -1.3|  95.1|  -2.1|eor|2010|   01| 01|    00|    30|\n",
      "+-----------+------------+-----+-----+------+------+------+------+---+----+-----+---+------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aus MESS_DATUM Jahr,Monat und Tag auslesen\n",
    "#Quelle: https://www.datasciencemadesimple.com/extract-first-n-and-last-n-character-in-pyspark/\n",
    "df_Temperatur  = df_Temperatur.withColumn(\"Jahr\", df_Temperatur.MESS_DATUM.substr(1,4))\n",
    "df_Temperatur  = df_Temperatur.withColumn(\"Monat\", df_Temperatur.MESS_DATUM.substr(5,2))\n",
    "df_Temperatur  = df_Temperatur.withColumn(\"Tag\", df_Temperatur.MESS_DATUM.substr(7,2))\n",
    "df_Temperatur  = df_Temperatur.withColumn(\"Stunde\", df_Temperatur.MESS_DATUM.substr(9,2))\n",
    "df_Temperatur  = df_Temperatur.withColumn(\"Minute\", df_Temperatur.MESS_DATUM.substr(11,2))\n",
    "df_Temperatur.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+-----+------+------+------+------+---+----+-----+---+------+------+----------------+\n",
      "|STATIONS_ID|MESS_DATUM  |  QN |PP_10|TT_10 |TM5_10|RF_10 |TD_10 |eor|Jahr|Monat|Tag|Stunde|Minute|ID              |\n",
      "+-----------+------------+-----+-----+------+------+------+------+---+----+-----+---+------+------+----------------+\n",
      "|2171       |201001010000|    3|960.8|  -1.3|  -1.3|  94.5|  -2.1|eor|2010|01   |01 |00    |00    |2171201001010000|\n",
      "|2171       |201001010010|    3|960.8|  -1.4|  -1.3|  94.6|  -2.2|eor|2010|01   |01 |00    |10    |2171201001010010|\n",
      "|2171       |201001010020|    3|960.7|  -1.4|  -1.3|  94.9|  -2.1|eor|2010|01   |01 |00    |20    |2171201001010020|\n",
      "|2171       |201001010030|    3|960.8|  -1.4|  -1.3|  95.1|  -2.1|eor|2010|01   |01 |00    |30    |2171201001010030|\n",
      "+-----------+------------+-----+-----+------+------+------+------+---+----+-----+---+------+------+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Neue IDSpalte erstellen\n",
    "#Quelle: https://sparkbyexamples.com/pyspark/pyspark-add-new-column-to-dataframe/#add%20column%20based%20on%20another%20column%20of%20dataframe\n",
    "df_Temperatur = df_Temperatur.withColumn(\"ID\",concat_ws('','STATIONS_ID','MESS_DATUM'))\n",
    "df_Temperatur.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unn√∂tige Spalten droppen\n",
    "#Quelle: https://sparkbyexamples.com/pyspark/pyspark-drop-column-from-dataframe/\n",
    "#Spalte TT_10 und TM5_10 umbenennen\n",
    "#Quelle: https://sparkbyexamples.com/pyspark/pyspark-rename-dataframe-column/\n",
    "df_Temperatur = df_Temperatur.drop('  QN','PP_10','RF_10','TD_10','eor').withColumnRenamed('TT_10','Temp_2m').withColumnRenamed('TM5_10','Temp_5cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |-- Temp_2m: string (nullable = true)\n",
      " |-- Temp_5cm: string (nullable = true)\n",
      " |-- Jahr: string (nullable = true)\n",
      " |-- Monat: string (nullable = true)\n",
      " |-- Tag: string (nullable = true)\n",
      " |-- Stunde: string (nullable = true)\n",
      " |-- Minute: string (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Temperatur.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading SolarData in DF \n",
    "df_Solar = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"false\").csv(folder_Solar+'*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29337980"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Solar.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+------+------+--------+-----+---+\n",
      "|STATIONS_ID|MESS_DATUM  |  QN |DS_10 |GS_10 |SD_10   |LS_10|eor|\n",
      "+-----------+------------+-----+------+------+--------+-----+---+\n",
      "|         73|201001010000|    3|  -999|  -999|   0.000|-999 |eor|\n",
      "|         73|201001010010|    3|  -999|  -999|   0.000|-999 |eor|\n",
      "|         73|201001010020|    3|  -999|  -999|   0.000|-999 |eor|\n",
      "+-----------+------------+-----+------+------+--------+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Struktur der DF Solar\n",
    "df_Solar.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- DS_10: string (nullable = true)\n",
      " |-- GS_10: string (nullable = true)\n",
      " |-- SD_10: string (nullable = true)\n",
      " |-- LS_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema des DF anzeigen\n",
    "df_Solar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |--   QN: string (nullable = true)\n",
      " |-- DS_10: string (nullable = true)\n",
      " |-- GS_10: string (nullable = true)\n",
      " |-- SD_10: float (nullable = true)\n",
      " |-- LS_10: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_Solar = df_Solar.withColumn('SD_10',df_Solar.SD_10.cast(FloatType()))\n",
    "df_Solar = df_Solar.withColumn('STATIONS_ID',df_Solar.STATIONS_ID.cast(IntegerType()))\n",
    "df_Solar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+------+------+-----+-----+---+----+-----+---+------+------+\n",
      "|STATIONS_ID|  MESS_DATUM|   QN| DS_10| GS_10|SD_10|LS_10|eor|Jahr|Monat|Tag|Stunde|Minute|\n",
      "+-----------+------------+-----+------+------+-----+-----+---+----+-----+---+------+------+\n",
      "|         73|201001010000|    3|  -999|  -999|  0.0| -999|eor|2010|   01| 01|    00|    00|\n",
      "|         73|201001010010|    3|  -999|  -999|  0.0| -999|eor|2010|   01| 01|    00|    10|\n",
      "|         73|201001010020|    3|  -999|  -999|  0.0| -999|eor|2010|   01| 01|    00|    20|\n",
      "|         73|201001010030|    3|  -999|  -999|  0.0| -999|eor|2010|   01| 01|    00|    30|\n",
      "+-----------+------------+-----+------+------+-----+-----+---+----+-----+---+------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Solar  = df_Solar.withColumn(\"Jahr\", df_Solar.MESS_DATUM.substr(1,4))\n",
    "df_Solar  = df_Solar.withColumn(\"Monat\", df_Solar.MESS_DATUM.substr(5,2))\n",
    "df_Solar  = df_Solar.withColumn(\"Tag\", df_Solar.MESS_DATUM.substr(7,2))\n",
    "df_Solar  = df_Solar.withColumn(\"Stunde\", df_Solar.MESS_DATUM.substr(9,2))\n",
    "df_Solar  = df_Solar.withColumn(\"Minute\", df_Solar.MESS_DATUM.substr(11,2))\n",
    "df_Solar.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+------+------+-----+-----+---+----+-----+---+------+------+--------------+\n",
      "|STATIONS_ID|MESS_DATUM  |  QN |DS_10 |GS_10 |SD_10|LS_10|eor|Jahr|Monat|Tag|Stunde|Minute|ID            |\n",
      "+-----------+------------+-----+------+------+-----+-----+---+----+-----+---+------+------+--------------+\n",
      "|73         |201001010000|    3|  -999|  -999|0.0  |-999 |eor|2010|01   |01 |00    |00    |73201001010000|\n",
      "|73         |201001010010|    3|  -999|  -999|0.0  |-999 |eor|2010|01   |01 |00    |10    |73201001010010|\n",
      "|73         |201001010020|    3|  -999|  -999|0.0  |-999 |eor|2010|01   |01 |00    |20    |73201001010020|\n",
      "|73         |201001010030|    3|  -999|  -999|0.0  |-999 |eor|2010|01   |01 |00    |30    |73201001010030|\n",
      "+-----------+------------+-----+------+------+-----+-----+---+----+-----+---+------+------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Neue IDSpalte erstellen\n",
    "df_Solar = df_Solar.withColumn(\"ID\",concat_ws('','STATIONS_ID','MESS_DATUM'))\n",
    "df_Solar.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |-- Sum_Sonnenscheindauer: float (nullable = true)\n",
      " |-- Jahr: string (nullable = true)\n",
      " |-- Monat: string (nullable = true)\n",
      " |-- Tag: string (nullable = true)\n",
      " |-- Stunde: string (nullable = true)\n",
      " |-- Minute: string (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Solar = df_Solar.drop('  QN','DS_10','GS_10','LS_10','eor').withColumnRenamed('SD_10','Sum_Sonnenscheindauer')\n",
    "df_Solar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F√ºr Hell und wei√ü im Januar --> Sommer hei√ü \n",
    "#Welche Daten brauch ich ? --> Temperatur des Sommers (Definiton Sommer?) + Sonnenstunden Januar + Schnee im Januar?\n",
    "#Finaler Datensatz = Gruppiert nach Jahr und Stations ID die Temperatur im Sommer und die Sonnenstunden im Januar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------------------+----+-----+---+------+------+--------------+\n",
      "|STATIONS_ID|  MESS_DATUM|Sum_Sonnenscheindauer|Jahr|Monat|Tag|Stunde|Minute|            ID|\n",
      "+-----------+------------+---------------------+----+-----+---+------+------+--------------+\n",
      "|         73|201001010000|                  0.0|2010|   01| 01|    00|    00|73201001010000|\n",
      "|         73|201001010010|                  0.0|2010|   01| 01|    00|    10|73201001010010|\n",
      "|         73|201001010020|                  0.0|2010|   01| 01|    00|    20|73201001010020|\n",
      "+-----------+------------+---------------------+----+-----+---+------+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bearbeiten der Datens√§tze, df_Solar und df_Temperature, damit nur die ben√∂tigten Informationen √ºbrig sind. \n",
    "df_Solar.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+--------+----+-----+---+------+------+----------------+\n",
      "|STATIONS_ID|  MESS_DATUM|Temp_2m|Temp_5cm|Jahr|Monat|Tag|Stunde|Minute|              ID|\n",
      "+-----------+------------+-------+--------+----+-----+---+------+------+----------------+\n",
      "|       2171|201001010000|   -1.3|    -1.3|2010|   01| 01|    00|    00|2171201001010000|\n",
      "|       2171|201001010010|   -1.4|    -1.3|2010|   01| 01|    00|    10|2171201001010010|\n",
      "|       2171|201001010020|   -1.4|    -1.3|2010|   01| 01|    00|    20|2171201001010020|\n",
      "+-----------+------------+-------+--------+----+-----+---+------+------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Temperatur.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2451799"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bearbeiten von df_Solar --> Nur noch Werte in DF mit Monat = 01\n",
    "#Quelle: https://stackoverflow.com/questions/52395986/remove-rows-from-dataframe-based-on-condition-in-pyspark\n",
    "df_Solar = df_Solar.filter(df_Solar.Monat == '01')\n",
    "df_Solar.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2325309"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bearbeiten von df_Solar --> Nur noch Werte bei denen Sonnenscheindauer <> -999\n",
    "df_Solar = df_Solar.filter(df_Solar.Sum_Sonnenscheindauer != -999)\n",
    "df_Solar.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gruppieren der Daten nach StationsID und Jahr\n",
    "#Quelle: https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/\n",
    "df_Solar_final = df_Solar.groupBy('STATIONS_ID','Jahr').agg(count('Sum_Sonnenscheindauer').alias('Anzahl_Eintraege'),(mean('Sum_Sonnenscheindauer')/0.06).alias('Anteil_Sonnenschein'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------------+-------------------+-------+\n",
      "|STATIONS_ID|Jahr|Anzahl_Eintraege|Anteil_Sonnenschein|     ID|\n",
      "+-----------+----+----------------+-------------------+-------+\n",
      "|        131|2018|            4464|0.15684363576600255|1312018|\n",
      "|         73|2014|            4464|0.12657183236103137| 732014|\n",
      "|        131|2011|            4464|0.24299581331816217|1312011|\n",
      "+-----------+----+----------------+-------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ID Column erstellen \n",
    "df_Solar_final = df_Solar_final.withColumn(\"ID\",concat_ws('','STATIONS_ID','Jahr'))\n",
    "df_Solar_final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2766987"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Monat == '06') | (df_Temperatur.Monat == '07') | (df_Temperatur.Monat == '08'))\n",
    "df_Temperatur.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+--------+----+-----+---+------+------+----------------+\n",
      "|STATIONS_ID|  MESS_DATUM|Temp_2m|Temp_5cm|Jahr|Monat|Tag|Stunde|Minute|              ID|\n",
      "+-----------+------------+-------+--------+----+-----+---+------+------+----------------+\n",
      "|       2171|201006010000|    9.9|     9.3|2010|   06| 01|    00|    00|2171201006010000|\n",
      "|       2171|201006010010|    9.9|     9.2|2010|   06| 01|    00|    10|2171201006010010|\n",
      "|       2171|201006010020|    9.8|     9.2|2010|   06| 01|    00|    20|2171201006010020|\n",
      "+-----------+------------+-------+--------+----+-----+---+------+------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Temperatur.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2668938"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temperatur = df_Temperatur.filter((df_Temperatur.Temp_2m != -999)|(df_Temperatur.Temp_5cm != -999))\n",
    "df_Temperatur.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------------+-----------------------+\n",
      "|STATIONS_ID|Jahr|Anzahl_Eintraege|Durchschnittstemperatur|\n",
      "+-----------+----+----------------+-----------------------+\n",
      "|       2171|2011|           13248|      16.63435235507249|\n",
      "|       2252|2013|           13248|      17.43346920289854|\n",
      "|       2252|2017|           13248|     17.966945954106286|\n",
      "+-----------+----+----------------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_Temperatur_final = df_Temperatur.groupBy('STATIONS_ID','Jahr').agg(count('Temp_2m').alias('Anzahl_Eintraege'),(mean('Temp_2m')).alias('Durchschnittstemperatur'))\n",
    "df_Temperatur_final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------------+-----------------------+--------+\n",
      "|STATIONS_ID|Jahr|Anzahl_Eintraege|Durchschnittstemperatur|      ID|\n",
      "+-----------+----+----------------+-----------------------+--------+\n",
      "|       2171|2011|           13248|      16.63435235507249|21712011|\n",
      "|       2252|2013|           13248|      17.43346920289854|22522013|\n",
      "|       2252|2017|           13248|     17.966945954106286|22522017|\n",
      "+-----------+----+----------------+-----------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ID Column erstellen \n",
    "df_Temperatur_final = df_Temperatur_final.withColumn(\"ID\",concat_ws('','STATIONS_ID','Jahr'))\n",
    "df_Temperatur_final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------------+-------------------+-------+-----------+----+----------------+-----------------------+-------+\n",
      "|STATIONS_ID|Jahr|Anzahl_Eintraege|Anteil_Sonnenschein|     ID|STATIONS_ID|Jahr|Anzahl_Eintraege|Durchschnittstemperatur|     ID|\n",
      "+-----------+----+----------------+-------------------+-------+-----------+----+----------------+-----------------------+-------+\n",
      "|        131|2011|            4464|0.24299581331816217|1312011|        131|2011|           13248|      16.98316727053144|1312011|\n",
      "|         73|2014|            4464|0.12657183236103137| 732014|         73|2014|           13248|      17.78292572463777| 732014|\n",
      "|         73|2011|            4464|0.20750074267939286| 732011|         73|2011|           13166|     17.856380069876913| 732011|\n",
      "+-----------+----+----------------+-------------------+-------+-----------+----+----------------+-----------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verbinden der beiden DF df_Temperature_final und df_Solar_Final\n",
    "#Quelle: https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/\n",
    "df_final = df_Solar_final.join(df_Temperatur_final,df_Solar_final.ID == df_Temperatur_final.ID,'left')\n",
    "df_final.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.03054000433767041"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correlation zwischen Sonnenschein und Durschsnittstemperatur\n",
    "df_final.stat.corr('Anteil_Sonnenschein','Durchschnittstemperatur')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
